\documentclass{article}
    % General document formatting
    %\usepackage[margin=0.7in]{geometry}
    %\usepackage[parfill]{parskip}
    %\usepackage[utf8]{inputenc}
    
    % Related to math
    \usepackage{amsmath, amssymb, bm}
    %\usepackage{amsmath,amssymb,amsfonts,amsthm}
    \DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
A Markov Decision Process (MDP) is defined by the following properties: a set of state $S$, actions $A$, dynamics $P_a$ that define the probablity of transiting between states given an action $a$, and a reward function $R$ that maps each state to a scalar value. A deterministic policy $\pi$ is a mapping from states to actions and a stochastic policy $\pi$ is a mapping from states to distibutions over actions from which an action is sampled. A value function $V^\pi$ maps states to the expected cumulative reward of a policy, with the following definition:
\[
  V^\pi(s) = R(s) +
  \begin{cases}
    0 & \text{if $s$ is terminal}\\ 
    \gamma \mathbb{E}_{s' \sim \bm{P}_\pi} \left[ V^\pi(s') \right] & \text{otherwise} \\ 
  \end{cases}
\]

where $\gamma \in (0, 1]$ and % chktex 9

\[
  \bm{P}_\pi = 
  \begin{cases}
    P_{\pi(s)}(s) & \text{if $\pi$ is deterministic} \\
    \mathbb{E}_{a \sim \pi(s)}\left[ P_a(s) \right] & \text{if $\pi$ is stochastic} \\
  \end{cases}
\]

Reinforcement learning studies algorithms for efficiently learning policies that maximize $V^\pi(s_0)$ where $s_0$ is some given start state. Current research focuses on learning the optimal policy for individual MDPs. However, a \textit{generally} intelligent agent must learn many MDPs simulateously, all sharing common states, dynamics, and actions, but parametrized by diverse reward functions. This corresponds to the intuition that animals and especially human beings learn to perform a wide range of tasks while the laws of physics and the range of muscle movements remain relatively constant. Moreover, these creatures do not postpone learning until pressed by nature and circumstance. One therefore observes small children exploring their environments with no motive but discovery.

This research attempts to formalize a system for learning in the absence of a specific reward. While an agent cannot learn an optimal policy in the absence of a reward function, it can learn dynamics and that is half the battle. 

A supplementary line of inquiry is the learning of a \textit{universal} value function that measures the probability of transitioning between arbitrary pairs of states:

\[ 
V_{g}^\pi(s) = 
  \begin{cases}
    1 & \text{if } s = g \\ 
    \mathbb{E}_{s' \sim \bm{P}_{\pi}}\left [ V_{g}^\pi(s') \right ] & \text{otherwise}
  \end{cases}
\]

Given $\pi$, $V_{g}^\pi$ is entirely determined by the transition function $P_a$. Now, consider the following variation of $V_g^\pi$:

\[
V_{g, n}^\pi(s) = 
	\begin{cases}
		0 & \text{if } n = 0 \\ 
		1 & \text{if } s = g \\ 
		\mathbb{E}_{s' \sim \bm{P}_{\pi}}\left [ V_{g, n-1}^\pi(s') \right ] & \text{otherwise}
	\end{cases}
\]

That is, $\pi$ terminates if it fails to reach $g$ within $n$ steps. Observe that $V_{g, n}^\pi(s)$ expresses the probability that $\pi$ reaches $g$, or more precisely, the probability that $g$ is among states $\bm{\tau}(s)$ that follow $s$:

\[ 
P\left(g \in \bm{\tau}(s)\right) =
\begin{cases}
    0 & \text{if } \bm{\tau}(s) = \emptyset \\ 
    1 & \text{if } s = g \\ 
    \mathbb{E}_{s' \sim \bm{P}_{\pi}}\left[ P \left( g \in \bm{\tau}(s') \right) \right] & \text{otherwise}
\end{cases}
\]

And by induction, 
\[
\mathbb{E}_{s' \sim \bm{P}_{\pi}}\left[ P \left( g \in \bm{\tau}(s') \right) \right] = \mathbb{E}_{s' \sim \bm{P}_{\pi}}\left[ V_{g, n-1}^\pi(s') \right]
\]

Note that $P_a(s) = V_{g, 1}^\pi(s)$ when $\pi(s) = a$. Thus universal value functions are a generalization from single-step to multi-step dynamics.

In light of these consideration, we define a set $M_{g}$ of MDPs that share common states $S$, actions $A$, and dynamics $P_a$, but that each have distinct reward functions defined as follows:
\[
  R_{g}(s) = 
  \begin{cases}
    1 & \text{if } s = g \\ 
    0 & \text{otherwise} \\ 
  \end{cases}
\]

This set of MDPs defines an environment in which the optimal policy for any reward function $R_{g}$ is the policy that maximizes $V_g(s)$, as previously defined, that is, the policy that most efficiently transitions between $s$ and $g$.

Now suppose we have some learned approximation $\hat{V}_{g}^\pi(s, \theta)$ of $V_{g}^\pi(s)$. In general, the suboptimality of any policy executed according to $\hat{V}$ will be a function of the approximation error. If we simply use square error to measure the approximation error, we have

\[
   \mathcal{L}_{V_g(s)}(\theta) = {({V_g^\pi(s) - \hat{V}_{g}^{\hat{\pi}}(s, \theta)})}^2
\]

\ldots 
\vspace{5mm}

Now suppose that we are using some gradient-based scheme of optimizing $\hat{V}$. Since 
\[
  \Delta \mathcal{L} = \lim_{|\Delta \theta| \to 0}\Delta \theta \cdot \mathcal{L}'(\theta)
\]
it follows that
\[
  \argmin \left[ \Delta \mathcal{L} \right] = \lim_{|\Delta \theta| \to 0}\argmin \left[ \Delta \theta \cdot \mathcal{L}'(\theta) \right]
\]
Of course, the value that satisfies this equation is some negative scaling of the gradient: $c\mathcal{L}'(\theta), c \to 0^-$. A common approach to this kind of problem is approximation by sampling:
\[
  \Delta \theta \leftarrow c\sum_{s \in B}\mathcal{L}_s'(\theta) \simeq c\mathcal{L}'(\theta)
\]

In practical settings, if $S$ is very large, it may be necessary to place some limit on the size of $B$. Moreover, it may be impossible to visit an arbitrary state $s$, and instead it may be necessary to navigate to $s$ from some given starting state $s_0$. Given these constraints, we wish to learn a policy $\pi$ that terminates after $n < |S|$ steps and visits those states that contribute most to the diminution of $\mathcal{L}$.

\section{To Do}
\begin{itemize}
\item We need an expression of $\mathcal{L}$ that enables us to state the contribution of each transition to it.
\item We need to figure out properties of this loss that enable us to minimize it most efficiently.
\end{itemize}

\section{Rough}

\begin{align}
   \mathcal{L}_{V_g(g)}(\theta) &= 0 \\
   \mathcal{L}_{V_g(s)}(\theta) &= {({\mathbb{E}_{s' \sim \bm{\hat{P}}(s)} {[{\hat{V}(s')}]}} - {\mathbb{E}_{s' \sim \bm{P}(s)} {[{V(s')}]}})}^2
 \end{align}

Given some parametrized approximation $\hat{V}_{ij}^{\theta}(s)$ of $V_{ij}(s)$ and some measure $\mathcal{L}_{ij}^s(\theta)$ of the error in $\hat{V}_{ij}^{\theta}(s)$, we define the global error of $\hat{V}$ as:
\[
   \mathcal{L}(\theta) = \sum_{s_1 \in S}\sum_{s_2 \in S}\mathcal{L}_{s_1,s_2}(\theta)
\text{, where }
   \mathcal{L}_{s_i, s_j}(\theta) = \mathcal{L}_{ij}^{s_i}(\theta)
\]


Now suppose that we are using some gradient-based scheme of optimizing $\hat{V}$. Since 
\[
  \Delta \mathcal{L} = \lim_{|\Delta \theta| \to 0}\Delta \theta \cdot \mathcal{L}'(\theta)
\]
it follows that
\[
  \argmin \left[ \Delta \mathcal{L} \right] = \lim_{|\Delta \theta| \to 0}\argmin \left[ \Delta \theta \cdot \mathcal{L}'(\theta) \right]
\]
Of course, the value that satisfies this equation is some negative scaling of the gradient: $c\mathcal{L}'(\theta), c \to 0^-$. A common approach to this kind of problem is approximation by sampling:
\[
  \Delta \theta \leftarrow c\sum_{s_1 \in B_1}\sum_{s_2 \in B_2}\mathcal{L}_{s_1,s_2}'(\theta) \simeq c\mathcal{L}'(\theta)
\]

In practical settings, if $S$ is very large, it may be necessary to place some limit on the number of states in $(B_1, B_2)$. Moreover, it may be impossible to visit an arbitrary state $s$, and instead it may be necessary to navigate to $s$ from some given starting state $s_0$. Given these observations and constraints, we wish to learn a policy $\pi$ that terminates after $n < |S|$ steps and visits those states that contribute most to the diminution of $\mathcal{L}$. Since the contribution of a series of states will be the sum of the contributions of each individual state, we first define the contribution of a single state to $\Delta \mathcal{L}$ as follows:

\[
	\mathcal{L}_{ij}^s(\theta) = 
  \begin{cases}
     & \text{if } s = s_j \\ 
    R_{ij}(s) + \mathbb{E}_{s' \sim P_{\pi_{ij}(s)}}\left [ V_{ij}(s') \right ] & \text{otherwise}
  \end{cases}
\]

\[
	R(s) = 
\]
%That is, the $\Delta \theta$ that contributes most to the diminution of $\mathcal{L}$ will be approximately collinear with
\end{document} % chktex 17
