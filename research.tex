\documentclass{article}
    % General document formatting
    %\usepackage[margin=0.7in]{geometry}
    %\usepackage[parfill]{parskip}
    %\usepackage[utf8]{inputenc}
    
    % Related to math
    \usepackage{amsmath, amssymb}
    %\usepackage{amsmath,amssymb,amsfonts,amsthm}
    \DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
A Markov Decision Process (MDP) is defined by the following properties: a set of state $S$, actions $A$, dynamics $P_a$ that define the probablity of transiting between states given an action $a$, and a reward function $R$ that maps each state to a scalar value. A deterministic policy $\pi$ is a mapping from states to actions and a stochastic policy $\pi$ is a mapping from states to distibutions over actions from which an action is sampled. A value function $V^\pi$ maps states to the expected cumulative reward of a policy, with the following definition:
\[
  V^\pi(s) = R(s) +
  \begin{cases}
    0 & \text{if $s$ is terminal}\\ 
    \gamma \mathbb{E}_{s' \sim \mathbf{P}_\pi} \left[ V^\pi(s') \right] & \text{otherwise} \\ 
  \end{cases}
\]

where $\gamma \in (0, 1)$ and

\[
  \mathbf{P}_\pi = 
  \begin{cases}
    P_{\pi(s)}(s) & \text{if $\pi$ is deterministic} \\
    \mathbb{E}_{a \sim \pi(s)}\left[ P_a(s) \right] & \text{if $\pi$ is stochastic} \\
  \end{cases}
\]

Reinforcement learning studies algorithms for efficiently learning policies that maximize $V^\pi(s_0)$ where $s_0$ is some given start state. Current research focuses on learning the optimal policy for individual MDPs. However, a \textit{generally} intelligent agent must learn many MDPs simulateously, all sharing common states, dynamics, and actions, but parametrized by diverse reward functions. This corresponds to the intuition that animals and especially human beings learn to perform a wide range of tasks while the laws of physics and the range of muscle movements remain relatively constant. Moreover, these creatures do not postpone learning until pressed by nature and circumstance. One therefore observes small children exploring their environments with no motive but discovery.

This research attempts to formalize a system for learning in the absence of a specific reward. A value function is determined by reward and dynamics. An optimal policy is determined by the value function. Therefore, while an agent cannot learn an optimal policy in the absence of a reward function, it can learn dynamics and that is half the battle. 

A supplementary line of inquiry is the learning of a \textit{universal} value function that measures the probability of transitioning between arbitrary pairs of states:

\[ 
V_{g}^\pi(s) = 
  \begin{cases}
    1 & \text{if } s = g \\ 
    \mathbb{E}_{s' \sim \mathbf{P}_{\pi}}\left [ V_{g}^\pi(s') \right ] & \text{otherwise}
  \end{cases}
\]

Given $\pi$, $V_{g}^\pi$ is entirely determined by the transition function $P_a$. Moreover, $P_a$ is a special case of $V_{g}^\pi$ in which $\pi$ is restricted to policies that terminate after a single step. Finally we observe that if $\pi$ is $V_{g}^\pi(s)$ expresses the probability that 

Therefore, if one learns the transition dynamics of an environment, all that remains to learn the optimal policy is the reward function, which is often easy to learn. It is therefore our contention that a great deal of useful information can be learned in an environment before a specific reward function is introduced. 

If it were possible to 




This document considers a set of Markov decision processes (MDPs), $M_{ij}$, that share common states $S$, actions $A$, and dynamics $P_a$, but that each have distinct reward functions defined as follows:
\[
  R_{ij}(S_i) = 
  \begin{cases}
    1 & \text{if } s_i = s_j \\ 
    0 & \text{otherwise} \\ 
  \end{cases}
\]

This set of MDPs defines an environment in which the optimal policy for any reward function $R_{ij}$ is the most efficient way to transition between $s_i$ and $s_j$ given the dynamics of the environment. Our objective is to determine the most efficient way to simulataneously learn all of these policies. We propose learning a universal value function $V_{ij}(s)$ defined as follows:
\[ 
V_{ij}(s) = 
  \begin{cases}
    R_{ij}(s_j) & \text{if } s = s_j \\ 
    R_{ij}(s) + \mathbb{E}_{s' \sim P_{\pi_{ij}(s)}}\left [ V_{ij}(s') \right ] & \text{otherwise}
  \end{cases}
\]

where $\pi_{ij}$ is the optimal policy for the MDP with reward function $R_{ij}$. Given some parametrized approximation $\hat{V}_{ij}^{\theta}(s)$ of $V_{ij}(s)$ and some measure $\mathcal{L}_{ij}^s(\theta)$ of the error in $\hat{V}_{ij}^{\theta}(s)$, we define the global error of $\hat{V}$ as:
\[
   \mathcal{L}(\theta) = \sum_{s_1 \in S}\sum_{s_2 \in S}\mathcal{L}_{s_1,s_2}(\theta)
\text{, where }
   \mathcal{L}_{s_i, s_j}(\theta) = \mathcal{L}_{ij}^{s_i}(\theta)
\]


Now suppose that we are using some gradient-based scheme of optimizing $\hat{V}$. Since 
\[
  \Delta \mathcal{L} = \lim_{|\Delta \theta| \to 0}\Delta \theta \cdot \mathcal{L}'(\theta)
\]
it follows that
\[
  \argmin \left[ \Delta \mathcal{L} \right] = \lim_{|\Delta \theta| \to 0}\argmin \left[ \Delta \theta \cdot \mathcal{L}'(\theta) \right]
\]
Of course, the value that satisfies this equation is some negative scaling of the gradient: $c\mathcal{L}'(\theta), c \to 0^-$. A common approach to this kind of problem is approximation by sampling:
\[
  \Delta \theta \leftarrow c\sum_{s_1 \in B_1}\sum_{s_2 \in B_2}\mathcal{L}_{s_1,s_2}'(\theta) \simeq c\mathcal{L}'(\theta)
\]

In practical settings, if $S$ is very large, it may be necessary to place some limit on the number of states in $(B_1, B_2)$. Moreover, it may be impossible to visit an arbitrary state $s$, and instead it may be necessary to navigate to $s$ from some given starting state $s_0$. Given these observations and constraints, we wish to learn a policy $\pi$ that terminates after $n < |S|$ steps and visits those states that contribute most to the diminution of $\mathcal{L}$. Since the contribution of a series of states will be the sum of the contributions of each individual state, we first define the contribution of a single state to $\Delta \mathcal{L}$ as follows:

\[
	\mathcal{L}_{ij}^s(\theta) = 
  \begin{cases}
     & \text{if } s = s_j \\ 
    R_{ij}(s) + \mathbb{E}_{s' \sim P_{\pi_{ij}(s)}}\left [ V_{ij}(s') \right ] & \text{otherwise}
  \end{cases}
\]

\[
	R(s) = 
\]
%That is, the $\Delta \theta$ that contributes most to the diminution of $\mathcal{L}$ will be approximately collinear with
\end{document}
